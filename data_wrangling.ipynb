{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12nx6q6z5ezu9cFD_oe0smE5bianCXjM9",
      "authorship_tag": "ABX9TyNekPgJj3xAbFB/HLcSAqmm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data wrangling over excel files\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sV7rqaCK_8gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install polars \n",
        "!pip install xlsx2csv"
      ],
      "metadata": {
        "id": "TNHaUwwR_XUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dcc695d-f27c-47b6-f320-3d6b02f437ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting polars\n",
            "  Downloading polars-0.15.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from polars) (4.4.0)\n",
            "Installing collected packages: polars\n",
            "Successfully installed polars-0.15.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xlsx2csv\n",
            "  Downloading xlsx2csv-0.8.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: xlsx2csv\n",
            "Successfully installed xlsx2csv-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "import xlrd as xls\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "eHaH_2wu_Qk1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## **A prior implementation**: descriptions are included"
      ],
      "metadata": {
        "id": "RkIwrnDeAC9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = \"/content/data_income/existencias-animales.xlsx\"\n",
        "file = \"/content/data_income/actividad-principal-region.xlsx\"\n",
        "file = \"/content/data_income/existencias-colmenas.xlsx\""
      ],
      "metadata": {
        "id": "_fJWqKscHnhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ini = lambda x, xs: [index+1 for (name, pibot, index) in zip(xs, [\"><\"]+xs, range(len(xs))) if name == pibot]\n",
        "get_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y]\n",
        "del_duplicate = lambda xs: [name for (name, pibot) in zip(xs, [\"><\"]+xs) if name != pibot]\n",
        "## Filling up empty space within the row\n",
        "filling_data = lambda xs: [ xs[i-1] if ( xs[i]==\"\" and i>0 ) else xs[i] for i in range(len(xs)) ]"
      ],
      "metadata": {
        "id": "Q8YpDuIEgyqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtaining indexes for sectoring data content"
      ],
      "metadata": {
        "id": "2TbM42va3NKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "work_book = xls.open_workbook(file)\n",
        "work_book = work_book.sheet_by_index(0)\n",
        "\n",
        "column = 1\n",
        "\n",
        "array_col = work_book.col_values(column)[1:]\n",
        "#array_col = del_duplicate(array_col)\n",
        "\n",
        "*a,ini,fin = get_indexes(\"\", array_col)\n",
        "\n",
        "## Filling up empty space within the row\n",
        "array_col = filling_data(array_col)\n",
        "\n",
        "## Keeping range that it cotains data\n",
        "array_col = array_col[ini: fin]\n",
        "print(array_col)"
      ],
      "metadata": {
        "id": "RcaoulQtH_5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217c8d62-2708-45d6-fce9-fa73aab2d97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Total Nacional - Región', 'Total Nacional', 'Región de Arica y Parinacota', 'Región de Tarapacá ', 'Región de Antofagasta', 'Región de Atacama', 'Región de Coquimbo', 'Región de Valparaíso', 'Región Metropolitana de Santiago', \"Región del Libertador General Bernardo O'Higgins\", 'Región del Maule', 'Región de Ñuble', 'Región del Biobío ', 'Región de La Araucanía', 'Región de Los Ríos', 'Región de Los Lagos', 'Región de Aysén del General Carlos Ibáñez del Campo', 'Región de Magallanes y de la Antártica Chilena']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting columns as arrays\n",
        "Sizing data of the columns\n",
        "Data normalization\n",
        "Storing arrays into fields\n",
        "\n",
        "This process will finish when there are no more columns with data"
      ],
      "metadata": {
        "id": "Ab-BkzLN-yNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fields = np.array(array_col)\n",
        "\n",
        "column=2\n",
        "while True:\n",
        "  try:\n",
        "    array_col = work_book.col_values(column)[1:]\n",
        "    #array_col = del_duplicate(array_col)\n",
        "    \n",
        "    ## Filling up empty space within the row\n",
        "    array_col = filling_data(array_col)\n",
        "\n",
        "    array_col = array_col[ini:fin]\n",
        "    column+=1\n",
        "\n",
        "    ## Data normalization\n",
        "    array_col = np.char.lower(array_col)\n",
        "    array_col = np.char.replace(array_col, \" \", \"_\")\n",
        "\n",
        "    #print(array_col)\n",
        "    fields = np.row_stack((fields, array_col))\n",
        "  except:\n",
        "    print(\"Ending column\")\n",
        "    break\n",
        "print(fields)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-I_w9djqheZ",
        "outputId": "ebcf1427-38a4-4fdf-f06f-cad9ddeec431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ending column\n",
            "[['Total Nacional - Región' 'Total Nacional'\n",
            "  'Región de Arica y Parinacota' 'Región de Tarapacá '\n",
            "  'Región de Antofagasta' 'Región de Atacama' 'Región de Coquimbo'\n",
            "  'Región de Valparaíso' 'Región Metropolitana de Santiago'\n",
            "  \"Región del Libertador General Bernardo O'Higgins\" 'Región del Maule'\n",
            "  'Región de Ñuble' 'Región del Biobío ' 'Región de La Araucanía'\n",
            "  'Región de Los Ríos' 'Región de Los Lagos'\n",
            "  'Región de Aysén del General Carlos Ibáñez del Campo'\n",
            "  'Región de Magallanes y de la Antártica Chilena']\n",
            " ['número_de_upa3,4' '5909.0' '6.0' '8.0' '7.0' '30.0' '297.0' '186.0'\n",
            "  '209.0' '387.0' '686.0' '444.0' '947.0' '1690.0' '456.0' '536.0' '29.0'\n",
            "  '1.0']\n",
            " ['número_de_colmenas' '333037.0' '53.0' '48.0' '25.0' '359.0' '6448.0'\n",
            "  '24658.0' '29572.0' '50291.0' '63887.0' '38936.0' '36048.0' '38489.0'\n",
            "  '16781.0' '27070.0' '366.0' '6.0']\n",
            " ['número_de_upa4' '4997.0' '5.0' '4.0' '5.0' '24.0' '230.0' '165.0'\n",
            "  '188.0' '353.0' '597.0' '386.0' '824.0' '1402.0' '398.0' '402.0' '22.0'\n",
            "  '1.0']\n",
            " ['número_de_colmenas' '306758.0' '49.0' '18.0' '21.0' '332.0' '5921.0'\n",
            "  '22541.0' '29298.0' '48967.0' '53961.0' '36789.0' '33951.0' '34933.0'\n",
            "  '15987.0' '23666.0' '318.0' '6.0']\n",
            " ['número_de_upa4' '995.0' '2.0' '4.0' '2.0' '6.0' '70.0' '22.0' '23.0'\n",
            "  '34.0' '95.0' '61.0' '130.0' '319.0' '65.0' '155.0' '7.0' '0.0']\n",
            " ['número_de_colmenas' '26279.0' '4.0' '30.0' '4.0' '27.0' '527.0'\n",
            "  '2117.0' '274.0' '1324.0' '9926.0' '2147.0' '2097.0' '3556.0' '794.0'\n",
            "  '3404.0' '48.0' '0.0']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing rows with lots of null values to get title for the dataframe"
      ],
      "metadata": {
        "id": "es8wkMZe46Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array_to_dataframe = np.transpose(fields)\n",
        "\n",
        "pos = 0\n",
        "normal_long = len(array_to_dataframe[pos])\n",
        "\n",
        "while True:\n",
        "  titles = array_to_dataframe[pos]\n",
        "  out_arr = np.char.count(titles, sub ='', start=1, end=1)\n",
        "  actual_long = np.sum(out_arr)\n",
        "  if not len(out_arr) == actual_long :\n",
        "    array_to_dataframe = array_to_dataframe[1:]\n",
        "  else:\n",
        "    break\n",
        "\n",
        "titles = np.char.lower(titles)\n",
        "titles = np.char.replace(titles, \" \", \"_\")\n",
        "\n",
        "for i in range(len(titles)):\n",
        "  titles[i] = titles[i] +\"_\"+ str(i)\n",
        "\n",
        "print(titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcLgd29nl3dQ",
        "outputId": "b0039bbe-43da-4f5f-c4ec-ef48a2f1dd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['total_nacional_-_región' 'número_de_upa3,4_1' 'número_de_colmenas_2'\n",
            " 'número_de_upa4_3' 'número_de_colmenas_4' 'número_de_upa4_5'\n",
            " 'número_de_colmenas_6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing the datafram"
      ],
      "metadata": {
        "id": "6it8Ti8O5PaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_polar = pl.from_numpy(data=array_to_dataframe, columns=list(titles),)\n",
        "pl.from_numpy\n",
        "df_polar = df_polar[1:]\n",
        "print(df_polar)\n",
        "\n",
        "file = file.replace(\"_income\",\"_outcome\")\n",
        "df_polar.write_csv(file.split('.')[0]+'.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KaU46lQn43G2",
        "outputId": "e2e56afd-2e9b-4479-c444-dbab4c482897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (17, 7)\n",
            "┌────────────┬────────────┬────────────┬──────────────┬──────────────┬──────────────┬──────────────┐\n",
            "│ total_naci ┆ número_de_ ┆ número_de_ ┆ número_de_up ┆ número_de_co ┆ número_de_up ┆ número_de_co │\n",
            "│ onal_-_reg ┆ upa3,4_1   ┆ colmenas_2 ┆ a4_3         ┆ lmenas_4     ┆ a4_5         ┆ lmenas_6     │\n",
            "│ ión        ┆ ---        ┆ ---        ┆ ---          ┆ ---          ┆ ---          ┆ ---          │\n",
            "│ ---        ┆ str        ┆ str        ┆ str          ┆ str          ┆ str          ┆ str          │\n",
            "│ str        ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "╞════════════╪════════════╪════════════╪══════════════╪══════════════╪══════════════╪══════════════╡\n",
            "│ Total      ┆ 5909.0     ┆ 333037.0   ┆ 4997.0       ┆ 306758.0     ┆ 995.0        ┆ 26279.0      │\n",
            "│ Nacional   ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Región de  ┆ 6.0        ┆ 53.0       ┆ 5.0          ┆ 49.0         ┆ 2.0          ┆ 4.0          │\n",
            "│ Arica y    ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "│ Parinacota ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Región de  ┆ 8.0        ┆ 48.0       ┆ 4.0          ┆ 18.0         ┆ 4.0          ┆ 30.0         │\n",
            "│ Tarapacá   ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Región de  ┆ 7.0        ┆ 25.0       ┆ 5.0          ┆ 21.0         ┆ 2.0          ┆ 4.0          │\n",
            "│ Antofagast ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "│ a          ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ ...        ┆ ...        ┆ ...        ┆ ...          ┆ ...          ┆ ...          ┆ ...          │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Región de  ┆ 456.0      ┆ 16781.0    ┆ 398.0        ┆ 15987.0      ┆ 65.0         ┆ 794.0        │\n",
            "│ Los Ríos   ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Región de  ┆ 536.0      ┆ 27070.0    ┆ 402.0        ┆ 23666.0      ┆ 155.0        ┆ 3404.0       │\n",
            "│ Los Lagos  ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Región de  ┆ 29.0       ┆ 366.0      ┆ 22.0         ┆ 318.0        ┆ 7.0          ┆ 48.0         │\n",
            "│ Aysén del  ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "│ General    ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "│ Carl...    ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Región de  ┆ 1.0        ┆ 6.0        ┆ 1.0          ┆ 6.0          ┆ 0.0          ┆ 0.0          │\n",
            "│ Magallanes ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "│ y de la    ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "│ Ant...     ┆            ┆            ┆              ┆              ┆              ┆              │\n",
            "└────────────┴────────────┴────────────┴──────────────┴──────────────┴──────────────┴──────────────┘\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PanicException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPanicException\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-bc58dc6e78c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_income\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"_outcome\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_polar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/polars/internals/dataframe/frame.py\u001b[0m in \u001b[0;36mwrite_csv\u001b[0;34m(self, file, has_header, sep, quote, batch_size, datetime_format, date_format, time_format, float_precision, null_value)\u001b[0m\n\u001b[1;32m   2031\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2033\u001b[0;31m         self._df.write_csv(\n\u001b[0m\u001b[1;32m   2034\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2035\u001b[0m             \u001b[0mhas_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPanicException\u001b[0m: called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: \"No such file or directory\" }"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## **Encapsulation and abstraction**: Object-oriented programming"
      ],
      "metadata": {
        "id": "HuJ4idPSFHAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FileItem:\n",
        "  def __init__(self, path):\n",
        "    self.path = path\n",
        "    self.list_files = []\n",
        "  \n",
        "  def collect_files(self):\n",
        "    # Get the list of all files and directories\n",
        "    self.list_files = os.listdir(self.path)\n",
        "    self.list_files = [ self.path +'/'+ x for x in self.list_files ]\n",
        "\n",
        "  def show_files(self):\n",
        "    for x in range(len(self.list_files)):\n",
        "      print(x+1, \"  \", self.list_files[x])"
      ],
      "metadata": {
        "id": "2Q2lohlmFkUT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CsvFileItem:\n",
        "  \n",
        "  get_ini = lambda x, xs: [index+1 for (name, pibot, index) in zip(xs, [\"><\"]+xs, range(len(xs))) if name == pibot == '']\n",
        "  get_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y]\n",
        "  del_duplicate = lambda xs: [name for (name, pibot) in zip(xs, [\"><\"]+xs) if name != pibot]\n",
        "  ## Filling up empty space within the row\n",
        "  filling_data = lambda xs: [ xs[i-1] if ( xs[i]==\"\" and i>0 ) else xs[i] for i in range(len(xs)) ]\n",
        "\n",
        "  def __init__(self, path_file) -> None:\n",
        "    self.path_file = path_file\n",
        "    self.name_file = path_file.split('/')[-1]\n",
        "    self.column=1\n",
        "    self.dataframe=1\n",
        "  \n",
        "  def get_column_workbook(self, column):\n",
        "    work_book = xls.open_workbook(self.path_file)\n",
        "    work_book = work_book.sheet_by_index(0)\n",
        "    array_col = work_book.col_values(column)[1:]\n",
        "\n",
        "    return array_col\n",
        "\n",
        "  def get_action_margin(self):\n",
        "    array_col = self.get_column_workbook(1)\n",
        "    *a,ini,fin = CsvFileItem.get_indexes(\"\", array_col)\n",
        "    return ini, fin\n",
        "\n",
        "  def filling_empty_cells(self, array_col):\n",
        "    ## Filling up empty space within the row\n",
        "    array_col = CsvFileItem.filling_data(array_col)\n",
        "    return array_col\n",
        "\n",
        "  def data_normalization(self, array_col):\n",
        "    ## Data normalization\n",
        "    array_col = np.char.lower(array_col)\n",
        "    array_col = np.char.strip(array_col)\n",
        "    array_col = np.char.replace(array_col, \" \", \"_\")\n",
        "    array_col = np.char.replace(array_col, \".\", \"\")\n",
        "    array_col = np.char.replace(array_col, \",\", \"\")\n",
        "    return array_col\n",
        "  \n",
        "\n",
        "  def get_columns_with_data(self):\n",
        "    ini, fin = self.get_action_margin()\n",
        "    array_col = self.get_column_workbook(self.column)\n",
        "    array_col = self.filling_empty_cells(array_col)\n",
        "    array_col = self.filling_empty_cells(array_col)\n",
        "    array_col = self.filling_empty_cells(array_col)\n",
        "    array_col = array_col[ini:fin]\n",
        "\n",
        "\n",
        "    column_collection = np.array(array_col)\n",
        "\n",
        "    self.column+=1\n",
        "\n",
        "    while True:\n",
        "      try:\n",
        "        array_col = self.get_column_workbook(self.column)\n",
        "        array_col = self.filling_empty_cells(array_col)\n",
        "        array_col = self.filling_empty_cells(array_col)\n",
        "        array_col = self.filling_empty_cells(array_col)\n",
        "        array_col = array_col[ini:fin]\n",
        "\n",
        "        self.column+=1\n",
        "\n",
        "        array_col = self.data_normalization(array_col)\n",
        "\n",
        "        column_collection = np.row_stack((column_collection, array_col))\n",
        "      except:\n",
        "        # \"Ending columns\"\n",
        "        break\n",
        "    return column_collection\n",
        " \n",
        "  def create_folder(self):\n",
        "    path_outcome = self.path_file.split(\"/\")[:-2] + ['data_outcome/']\n",
        "    path_outcome = \"/\".join(path_outcome)\n",
        "    \n",
        "    # Check whether the specified path exists or not\n",
        "    isExist = os.path.exists(path_outcome)\n",
        "    if not isExist:\n",
        "      # Create a new directory because it does not exist\n",
        "      os.makedirs(path_outcome)\n",
        "    \n",
        "    return path_outcome\n",
        "\n",
        " \n",
        "  def get_titles_dataframe(self):\n",
        "    array_to_dataframe = np.transpose(self.get_columns_with_data())\n",
        "    ## Going over rows\n",
        "    titles=[]\n",
        "    pos = 0\n",
        "    normal_long = len(array_to_dataframe[pos])\n",
        "\n",
        "    while True:\n",
        "      titles = array_to_dataframe[pos]\n",
        "      out_arr = np.char.count(titles, sub ='', start=1, end=1)\n",
        "      actual_long = np.sum(out_arr)\n",
        "      out_arr = out_arr.tolist()\n",
        "      if not len(out_arr) == actual_long :\n",
        "        array_to_dataframe = array_to_dataframe[1:]\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    #titles = self.data_normalization(titles)\n",
        "    titles = titles.tolist()\n",
        "\n",
        "    #if it would have many occurences, just in time \n",
        "    if len(titles) >0 :\n",
        "      titles = [ name+\"_\"+str(i) for (name,i) in zip(titles, range(len(titles)))  ]\n",
        "\n",
        "    return titles, array_to_dataframe\n",
        "\n",
        "\n",
        "  def save_dataframe_csv(self):\n",
        "    titles, array_to_dataframe = self.get_titles_dataframe()\n",
        "\n",
        "    df_polar = pl.from_numpy(data=array_to_dataframe, columns=list(titles),)\n",
        "    \n",
        "    df_polar = df_polar[1:]\n",
        "    self.dataframe = df_polar\n",
        "\n",
        "    path_outcome = self.create_folder()\n",
        "    path_outcome = path_outcome + self.name_file.split('.')[0]+'.csv'\n",
        "    print(path_outcome)\n",
        "\n",
        "    df_polar.write_csv(path_outcome)\n"
      ],
      "metadata": {
        "id": "bFBIWs06Hejj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class' instance for \"FileItem\" and \"CsvFileItem\""
      ],
      "metadata": {
        "id": "4hZVt--laz2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = FileItem('/content/data_income')\n",
        "files.collect_files()\n",
        "files.show_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0YKFvzWaoNa",
        "outputId": "bb6f2a97-e3f8-4afc-cae9-cebc1436b835"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    /content/data_income/numero-upa-practicas-mejoramiento-suelo.xlsx\n",
            "2    /content/data_income/numero-productores-tramos-edad.xlsx\n",
            "3    /content/data_income/existencias-colmenas.xlsx\n",
            "4    /content/data_income/numero-upa-orientación-colmenas.xlsx\n",
            "5    /content/data_income/01_numero_superficie_de_upa_censadas_regional-xlsx.xlsx\n",
            "6    /content/data_income/numero-superficie-de-upa-forestal-regional-comunal.xlsx\n",
            "7    /content/data_income/numero-de-upa-y-superficie-por-categoría-de-uso-del-suelo.xlsx\n",
            "8    /content/data_income/numero-productores-pueblos-originarios.xlsx\n",
            "9    /content/data_income/numero-persona-natural-tipo-tenencia.xlsx\n",
            "10    /content/data_income/tamaño-upa-región-comuna.xlsx\n",
            "11    /content/data_income/actividad-principal-región.xlsx\n",
            "12    /content/data_income/numero-superficie-de-upa-frutales-regional-comunal.xlsx\n",
            "13    /content/data_income/numero-superficie-de-upa-aire-libre-bajo-cubierta-regional-comunal.xlsx\n",
            "14    /content/data_income/numero-personas-administradoras.xlsx\n",
            "15    /content/data_income/superficie-principal-sistema-riego.xlsx\n",
            "16    /content/data_income/superficie-categoría-cultivo-región-comuna.xlsx\n",
            "17    /content/data_income/trabajo-agrícola.xlsx\n",
            "18    /content/data_income/existencias-animales.xlsx\n",
            "19    /content/data_income/numero-superficie-de-upa-riego-secano-regional-comunal.xlsx\n",
            "20    /content/data_income/tipo-gestión-región-comuna.xlsx\n",
            "21    /content/data_income/numero-superficie-de-upa-bosque-nativo-regional-comunal.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obj = CsvFileItem(files.list_files[0])\n",
        "obj.save_dataframe_csv()\n",
        "print(obj.dataframe)"
      ],
      "metadata": {
        "id": "7MhTgN8QbDyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = CsvFileItem(files.list_files[1])\n",
        "obj.save_dataframe_csv()\n",
        "print(obj.dataframe)"
      ],
      "metadata": {
        "id": "_7xg96jcj3tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = CsvFileItem(files.list_files[2])\n",
        "obj.save_dataframe_csv()\n",
        "print(obj.dataframe.head())"
      ],
      "metadata": {
        "id": "KLLQAtPrj_Ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = CsvFileItem(files.list_files[3])\n",
        "obj.save_dataframe_csv()\n",
        "print(obj.dataframe.head())"
      ],
      "metadata": {
        "id": "8Ao2FOUPrA1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del obj\n",
        "obj = CsvFileItem(files.list_files[4])\n",
        "print(obj.path_file)\n",
        "\n",
        "a, b=obj.get_action_margin()\n",
        "print(a,b)\n",
        "\n",
        "obj.get_columns_with_data()\n",
        "print(obj.get_columns_with_data())\n",
        "\n",
        "#print(list(map(list, zip(*obj.get_columns_with_data()))))\n",
        "\n",
        "\n",
        "#obj.get_titles_dataframe()\n",
        "\n",
        "#obj.save_dataframe_csv()\n",
        "#print(obj.dataframe.head())\n",
        "  "
      ],
      "metadata": {
        "id": "8yf4lXRIsIYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = CsvFileItem(files.list_files[5])\n",
        "obj.save_dataframe_csv()\n",
        "print(obj.dataframe.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQpsb8Nxvhob",
        "outputId": "d2fb7591-d9ed-4e25-df34-7e1dc93cbfd2"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data_outcome/numero-superficie-de-upa-forestal-regional-comunal.csv\n",
            "shape: (5, 5)\n",
            "┌──────────────────┬────────────────┬────────────────┬───────────────────┬─────────────────────────┐\n",
            "│ Total Nacional - ┆ comuna_4_5_1   ┆ especie_6_2    ┆ total             ┆ total                   │\n",
            "│ Región_0         ┆ ---            ┆ ---            ┆ número_de_upa78_3 ┆ superficie_cultivo_(ha) │\n",
            "│ ---              ┆ str            ┆ str            ┆ ---               ┆ _4                      │\n",
            "│ str              ┆                ┆                ┆ str               ┆ ---                     │\n",
            "│                  ┆                ┆                ┆                   ┆ str                     │\n",
            "╞══════════════════╪════════════════╪════════════════╪═══════════════════╪═════════════════════════╡\n",
            "│ Total Nacional   ┆ total_nacional ┆ total          ┆ 303640            ┆ 2068400835026186        │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Total Nacional   ┆ total_nacional ┆ sin_clasificar ┆ 4050              ┆ 29009570000005          │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Total Nacional   ┆ total_nacional ┆ acacia         ┆ 1630              ┆ 43545                   │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Total Nacional   ┆ total_nacional ┆ álamo          ┆ 3100              ┆ 6201173000008826        │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ Total Nacional   ┆ total_nacional ┆ aromo          ┆ 8500              ┆ 4691985000003902        │\n",
            "└──────────────────┴────────────────┴────────────────┴───────────────────┴─────────────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "for index in range(5,len(files.list_files)):\n",
        "  obj = CsvFileItem(files.list_files[index])\n",
        "  obj.save_dataframe_csv()\n",
        "  print(obj.dataframe.head())\n"
      ],
      "metadata": {
        "id": "FHYiHGhisWv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New procedures and methods for CsvFileItem class"
      ],
      "metadata": {
        "id": "Mxs9AT-Hd8UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Removing empty rows \n",
        "def remove_empty_rows(work_book):\n",
        "  nrows = work_book.nrows\n",
        "  ncols = work_book.ncols\n",
        "\n",
        "  for i in range( nrows):\n",
        "    nempty = work_book.count('')\n",
        "    if nempty == ncols:\n",
        "      work_book.remove_row(i)\n",
        "  \n",
        "  return work_book\n",
        "\n",
        "\n",
        "## extextending values toward blank cells\n",
        "def extending_values_blanckspaces(work_book):\n",
        "  nrows = work_book.nrows\n",
        "  ncols = work_book.ncols\n",
        "  rack_rows = np.array(np.ones(ncols))\n",
        "  for i in range(nrows):\n",
        "    array_row = work_book.row_values(i)\n",
        "\n",
        "    for index in range(len(array_row)):##going by columns\n",
        "      if array_row[index] == '' and index > 0:\n",
        "        work_book.row_values(i)[index] = work_book.row_values(i)[index-1]\n",
        "  #     array_row[index] = array_row[index-1]\n",
        "  #  array_row = np.array(array_row)\n",
        "    \n",
        "  #  rack_rows = np.row_stack( ( rack_rows , array_row ) )\n",
        "  #rack_rows = rack_rows[1:]\n",
        "  #return rack_rows\n",
        "  return work_book \n",
        "\n",
        "\n",
        "## removing empty columns\n",
        "def remove_empty_cols(work_book):\n",
        "  nrows = work_book.nrows\n",
        "  ncols = work_book.ncols\n",
        "  \n",
        "  for i in range(1, ncols):\n",
        "    nempty = work_book[:, i-1:i]\n",
        "    nempty = np.transpose(nempty).tolist()[0].count('')\n",
        "    if float(nempty/nrows) >= 0.75 :\n",
        "      work_book = work_book[:, i:]\n",
        "  \n",
        "  return work_book\n",
        "\n",
        "\n",
        "## Verifying if this column is empty\n",
        "def is_empty_col(work_book_column):\n",
        "  ncols = len(work_book_column)\n",
        "  nempty = work_book_column[:, 0:1]\n",
        "  nempty = np.transpose(nempty).tolist()[0].count('')\n",
        "  if nempty == ncols :\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "## Verifying if this row is empty\n",
        "def is_empty_row(work_book_row):\n",
        "  nempty = work_book_row.count('')\n",
        "  if nempty == len(work_book_row):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "## Removing comments head 1. By a row with contente between before and after blank line 2 and if it would be the first and unique column with data.\n",
        "def remove_head_comments(work_book):\n",
        "  nrows = work_book.nrows\n",
        "  for index in range(0,len(work_book)):\n",
        "    row = work_book[index]\n",
        "    if not is_empty_row(row):\n",
        "      if index == 0 and index < nrows-1:\n",
        "        row_ahead = work_book[index+1]\n",
        "        if is_empty_row(row_ahead):\n",
        "          columns = len(work_book[index])\n",
        "          work_book[index] = ['']*columns\n",
        "\n",
        "      elif index > 0 and index < nrows-1:\n",
        "        row_back = work_book[index-1]\n",
        "        row_ahead = work_book[index+1]\n",
        "        if is_empty_row(row_ahead) and is_empty_row(row_back)  :\n",
        "          columns = len(work_book[index])\n",
        "          work_book[index] = ['']*columns\n",
        "  \n",
        "  remove_empty_rows(work_book)\n",
        "  return work_book\n",
        "\n",
        "\n",
        "## Removing comments bottom 2. By couting from last row to up and verifying if it would be the first and unique column with data\n",
        "##Pendient\n",
        "\n",
        "def show_work_book(work_book):\n",
        "  \"\"\" show work_rows' rows \"\"\"\n",
        "  for i in work_book.get_rows():\n",
        "    print(i)\n",
        "\n",
        "def to_ndimention_xls(work_book):\n",
        "  \"\"\" Proccessing data and giving Numpy array format \"\"\"\n",
        "  rack_book = np.array(['']*work_book.ncols, dtype='<U32')\n",
        "  for i in range(work_book.nrows):\n",
        "    rack_book = np.vstack( ( rack_book, np.array(work_book.row_values(i), dtype='<U32') ) )\n",
        "  return rack_book[1:]\n",
        "\n",
        "################################################################################################################################################\n",
        "print(files.list_files[7])\n",
        "work_book = xls.open_workbook(files.list_files[7])\n",
        "work_book = work_book.sheet_by_index(0)\n",
        "\n",
        "\n",
        "for i in to_ndimention_xls(work_book):\n",
        "  print(i.tolist().count(''))"
      ],
      "metadata": {
        "id": "aAatn8wAYamA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}